<div align="center">

<h1>Text2Performer: Text-Driven Human Video Generation</h1>

<div>
    <a href="https://yumingj.github.io/" target="_blank">Yuming Jiang</a><sup>1</sup>,</span>
    <a href="https://williamyang1991.github.io/" target="_blank">Shuai Yang</a><sup>1</sup>,</span>
    <a>Tong Liang Koh</a><sup>1</sup>
    <a href="https://wywu.github.io/" target="_blank">Wayne Wu</a><sup>2</sup>
    <a href="https://www.mmlab-ntu.com/person/ccloy/" target="_blank">Chen Change Loy</a><sup>1</sup>
    <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a><sup>1</sup>
</div>
<div>
    <sup>1</sup>S-Lab, Nanyang Technological University&emsp; <sup>2</sup>Shanghai AI Laboratory
</div>

</br>
</br>

<strong>Text2Performer synthesizes human videos by taking the text descriptions as the only input.</strong>

<div style="width: 100%; text-align: center; margin:auto;">
    <img style="width:100%" src="img/teaser.png">
</div>

:open_book: For more visual results, go checkout our <a href="https://yumingj.github.io/projects/Text2Performer.html" target="_blank">project page</a>


## :newspaper_roll: License

Distributed under the S-Lab License. See `LICENSE` for more information.


